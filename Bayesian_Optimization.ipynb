{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayesian_Optimization",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShashankKamath/AutoML/blob/master/Bayesian_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkvTFS8vwQN5",
        "colab_type": "code",
        "outputId": "1202c933-cd31-4559-d823-76a50d30627d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchsummary  import summary\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import Linear\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "!pip install bayesian-optimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0c/173ac467d0a53e33e41b521e4ceba74a8ac7c7873d7b857a8fbdca88302d/bayesian-optimization-1.0.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.16.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.3.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.13.2)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.0.1-cp36-none-any.whl size=10032 sha256=1ba08bc4d36949fc54c038acaa0a48d7ec4f313f896a76fc1892ffada85eaf91\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/0d/3b/6b9d4477a34b3905f246ff4e7acf6aafd4cc9b77d473629b77\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shQyP-mdxDkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 3\n",
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87nOBMKuxAlB",
        "colab_type": "code",
        "outputId": "153b2069-39c2-456f-b501-fa5801a22bab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "\n",
        "train_ds= torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,)) ]))\n",
        "\n",
        "test_ds=torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))  ]))\n",
        "\n",
        "train_loader = DataLoader(train_ds,batch_size=batch_size_train, shuffle=True)\n",
        "test_loader = DataLoader(test_ds,batch_size=batch_size_test, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /files/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 9093328.41it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /files/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 137452.56it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /files/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2324308.74it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /files/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 53388.78it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_8I0qn5BnSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.drop_out = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.drop_out(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "model=ConvNet()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwN_mhB9zN8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define function to check model accuracy\n",
        "def accuracy(out, yb):\n",
        "    preds = torch.argmax(out, dim=1)\n",
        "    return (preds == yb).float().mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWwgeFmvGS1J",
        "colab_type": "code",
        "outputId": "0d366afe-c2f4-43c9-d854-7b2baadaf966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def get_model_accuracy():\n",
        "  tot_acc = 0\n",
        "  avg_acc = 0\n",
        "  \n",
        "  # Set model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  for xbt, ybt in test_loader:\n",
        "\n",
        "    pred = model(xbt.to(device))\n",
        "    tot_acc += accuracy(pred,ybt.to(device))\n",
        "\n",
        "  avg_acc = tot_acc / len(test_loader)\n",
        "  \n",
        "  return avg_acc.item()\n",
        "\n",
        "\n",
        "# Print accuracy of model\n",
        "print(\"The average accuracy is: \" + str(get_model_accuracy()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The average accuracy is: 0.9795999526977539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNvlhzBHJIht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNtrain(nn.Module):\n",
        "    def __init__(self,numLayers,channel):\n",
        "        super(CNNtrain, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        i=0\n",
        "        self.layers.append(nn.Conv2d(channel[i], channel[i+1], kernel_size=3,padding=0))\n",
        "        i+=1\n",
        "        for j in range(numLayers - 2):\n",
        "          self.layers.append(nn.Conv2d(in_channels=channel[i], out_channels=channel[i+1], kernel_size=3,padding=0))\n",
        "          i+=1\n",
        "        self.layers.append(nn.Conv2d(channel[i], channel[i+1], kernel_size=3,padding=0))\n",
        "        i+=2\n",
        "        self.layers.append(nn.Linear(channel[i],channel[i+1]))\n",
        "        i+=1\n",
        "        self.layers.append(nn.Linear(channel[i],channel[i+1]))\n",
        "#         print(self)\n",
        "\n",
        "    def forward(self, x):\n",
        "      y = x\n",
        "#       print(y.shape)\n",
        "      y = F.relu(F.max_pool2d(self.layers[0](y),kernel_size=(2, 2)))   \n",
        "#       print(y.shape)\n",
        "      for i in range(len(self.layers)-4):\n",
        "        y = F.relu(F.max_pool2d(self.layers[i+1](y),kernel_size=(2, 2))) #self.layers[i+1](y)\n",
        "#         print(y.shape)\n",
        "      y = F.relu(F.max_pool2d(self.layers[-3](y),kernel_size=(2, 2)))\n",
        "#       print(y.size())\n",
        "      y = y.view(-1,channel[-3])\n",
        "#       print(\"BFC\",y.size())\n",
        "      y = F.relu(self.layers[-2](y))\n",
        "#       print(\"FC1\",y.size())\n",
        "      y = self.layers[-1](y)\n",
        "#       print(\"FC2\",y.size())\n",
        "      \n",
        "      return F.log_softmax(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z8plt-HJJhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "class CNNpossible(nn.Module):\n",
        "    def __init__(self,numLayers):\n",
        "        super(CNNpossible, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        channel.append(1)\n",
        "        channel.append(random.randint(5,10))\n",
        "        self.layers.append(nn.Conv2d(channel[-2], channel[-1], kernel_size=3,padding=0))\n",
        "        for i in range(numLayers - 2):\n",
        "          channel.append(random.randint(5,10))\n",
        "          self.layers.append(nn.Conv2d(in_channels=channel[-2], out_channels=channel[-1], kernel_size=3,padding=0))  \n",
        "        channel.append(1)\n",
        "        self.layers.append(nn.Conv2d(channel[-2], channel[-1], kernel_size=3,padding=0))\n",
        "        channel.append(25)\n",
        "        channel.append(50)\n",
        "        channel.append(10)\n",
        "        self.layers.append(nn.Linear(channel[-3],channel[-2]))\n",
        "        self.layers.append(nn.Linear(channel[-2],channel[-1]))\n",
        "#         print(\"Channel Details:\",channel)\n",
        "#         print(self)\n",
        "\n",
        "    def forward(self, x):\n",
        "      y = x\n",
        "#       print(y.shape)\n",
        "      try:\n",
        "        y = F.relu(F.max_pool2d(self.layers[0](y),kernel_size=(2, 2)))   \n",
        "#         print(\"First Layer\",y.shape)\n",
        "        for i in range(len(self.layers)-4):\n",
        "          y = F.relu(F.max_pool2d(self.layers[i+1](y),kernel_size=(2, 2))) #self.layers[i+1](y)\n",
        "#           print(\"Middle Layer\",y.shape)\n",
        "        y = F.relu(F.max_pool2d(self.layers[-3](y),kernel_size=(2,2)))\n",
        "#         print(\"Last Layer\",y.size())\n",
        "\n",
        "        y = y.view(-1,(y.size(1)*y.size(2)*y.size(3)))\n",
        "#         print(\"Before FC\",y.size())\n",
        "\n",
        "        y = F.relu(self.layers[-2](y))\n",
        "#         print(\"FC1\",y.size())\n",
        "      \n",
        "        y = self.layers[-1](y)\n",
        "#         print(\"FC2\",y.size())\n",
        "        \n",
        "        return True\n",
        "      \n",
        "      except Exception as e:\n",
        "#         print(\"Entered Exception\",str(e))  \n",
        "        if \"Kernel size can't be greater than actual input size\" in str(e):\n",
        "#           print(\"Kernel Size exceed- Not possible config\")\n",
        "          return False #torch.tensor([0])\n",
        "        \n",
        "        \n",
        "        elif \"is invalid for input of size\" or \"size mismatch\" in str(e):\n",
        "#           print(\"Fully Connected Change required\")\n",
        "#           y = y.view(-1,(y.size(1)*y.size(2)*y.size(3)))\n",
        "          y = y.view(y.size(0),-1)\n",
        "          channel[-3]=y.size(1)\n",
        "          channel[-2]=random.randint(5,50)\n",
        "#           print(\"Channel Details after FC:\",channel)\n",
        "          return True\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC1T0E74JLQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGYR8NGxJOrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model2,optimizer2,epoch):\n",
        "  model2.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer2.zero_grad()\n",
        "    output = model2(data)\n",
        "    loss = F.nll_loss(output, target)  \n",
        "    loss.backward()\n",
        "    optimizer2.step()\n",
        "    if batch_idx % log_interval == 0:\n",
        "#       print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "#         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "#         100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append((batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "      torch.save(model2.state_dict(), 'model2.pth')\n",
        "      torch.save(optimizer2.state_dict(), 'optimizer2.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxXhYfBnK6rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model1):\n",
        "  model1.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = model1(data)\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  accuracy= 100. * correct / len(test_loader.dataset)\n",
        "#   print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "#     test_loss, correct, len(test_loader.dataset),\n",
        "#     100. * correct / len(test_loader.dataset)))\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1wrWe4MKezV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_possible(model):\n",
        "    for data, target in test_loader:\n",
        "      result = model(data)\n",
        "      if result==False:\n",
        "#         print(\"Correct Layer:\",num_layer)\n",
        "#         print(\"Channel: \",channel)\n",
        "        y.append([0])\n",
        "        x.append([num_layer,channel])\n",
        "#         print(\"X\",x)\n",
        "#         print(\"Y\",y)\n",
        "        break\n",
        "      elif result==True:\n",
        "#         print(\"Correct Layer:\",num_layer)\n",
        "#         print(\"Channel: \",channel)  \n",
        "        x.append([num_layer,channel])\n",
        "        model2 = CNNtrain(num_layer,channel)\n",
        "        optimizer2 = optim.SGD(model2.parameters(), lr=learning_rate,momentum=momentum)  \n",
        "#         accuracy=test(model2)\n",
        "#         print(\"Accuracy Before Training:\",accuracy)\n",
        "        for epoch in range(1, n_epochs + 1):\n",
        "          train(model2,optimizer2,epoch)\n",
        "        accuracy=int(test(model2))\n",
        "        print(\"Accuracy after Training:\",accuracy)\n",
        "        y.append([accuracy])\n",
        "#         print(\"X\",x)\n",
        "#         print(\"Y\",y)         \n",
        "        break\n",
        "                 \n",
        "x=[]\n",
        "y=[]\n",
        "n_epochs=5\n",
        "for run in range(1,2):\n",
        "  num_layer=random.randint(1,5)\n",
        "  channel=[]\n",
        "  model = CNNpossible(num_layer)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)\n",
        "  test_possible(model)\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzKpm8QwDLH9",
        "colab_type": "text"
      },
      "source": [
        "Bayesian Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly-CVxKcDM2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def obj_func(lr, bs, epochs , optimizervalue, loss):\n",
        "      \n",
        "      # We need to round off bs and epochs because Gaussian processes cannot deal with discrete variables \n",
        "      bs = int(bs)\n",
        "      # print(\"Batch Size\",bs)\n",
        "      epochs = int(epochs)\n",
        "      # print(\"Number of epochs\",epochs)\n",
        "      optimizervalue=int(round(optimizervalue))\n",
        "      # print(\"Optimizer Value\",optimizervalue)\n",
        "      loss=int(round(loss))\n",
        "      # print(\"Loss Function Value\",loss)\n",
        "\n",
        "      train_loader = DataLoader(train_ds,batch_size=bs, shuffle=True)\n",
        "      test_loader = DataLoader(test_ds,batch_size=bs, shuffle=True)\n",
        "      \n",
        "      \n",
        "      if optimizervalue==1:\n",
        "          optimization = optim.Adam(model.parameters(), lr = lr)\n",
        "          print(\"Adam Optimizer\")\n",
        "      elif optimizervalue==2:\n",
        "          optimization = optim.SGD(model.parameters(),lr=lr)\n",
        "          print(\"SGD Optimizer\")\n",
        "      elif optimizervalue==3:\n",
        "          optimization = optim.RMSprop(model.parameters(),lr=lr)\n",
        "          print(\"RMSProp Optimizer\")\n",
        "\n",
        "      if loss==1:\n",
        "        loss_func = F.cross_entropy\n",
        "        print(\"Cross Entropy Loss\")\n",
        "      elif loss==2:\n",
        "        loss_func = F.nll_loss\n",
        "        print(\"NLL Loss\")\n",
        "      elif loss==3:\n",
        "        loss_func = F.mse_loss\n",
        "        print(\"MSE Loss\")\n",
        "\n",
        "\n",
        "      #  loss MSELoss,CrossEntropyLoss, NLLLoss\n",
        "      for epoch in range(epochs):\n",
        "    \n",
        "        for xb, yb in train_loader:\n",
        "        \n",
        "            # .to(device) moves torch.Tensor objects to the GPU for faster training\n",
        "        \n",
        "            preds = model(xb.to(device))\n",
        "            loss = loss_func(preds, yb.to(device))\n",
        "            acc = accuracy(preds,yb.to(device))\n",
        "        \n",
        "            loss.backward()\n",
        "            optimization.step()\n",
        "            optimization.zero_grad()\n",
        "        \n",
        "        print(\"Loss: \" + str(loss.item()) + \"\\t \\t Accuracy: \" + str(100 * acc.item()))\n",
        "\n",
        "      acc = get_model_accuracy()\n",
        "      \n",
        "      return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aTkKjHLDNwS",
        "colab_type": "code",
        "outputId": "750e2a06-6a97-43ad-e2a9-d1d09b8990ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        }
      },
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "from bayes_opt import UtilityFunction\n",
        "\n",
        "utility = UtilityFunction(kind=\"ucb\", kappa=2.5, xi=0.0)\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'lr': (1e-4, 1e-2), \n",
        "           'bs': (64, 512), \n",
        "           'epochs': (1,3), \n",
        "           'optimizervalue':(1,3),\n",
        "           'loss':(1,2) }\n",
        "          #  'layers':(1,4), \n",
        "          #  'units':(1,10)}\n",
        "\n",
        "model = ConvNet()\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=obj_func,\n",
        "    pbounds=pbounds,\n",
        "    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
        "    random_state=1,\n",
        ")\n",
        "\n",
        "\n",
        "# for _ in range(5):\n",
        "#     next_point = optimizer.suggest(utility)\n",
        "#     target = obj_func(**next_point)\n",
        "#     optimizer.register(params=next_point, target=target)\n",
        "#     print(target, next_point)\n",
        "# print(optimizer.max)\n",
        "\n",
        "optimizer.maximize(init_points=2, n_iter=3,)\n",
        "print(optimizer.max)\n",
        "\n",
        "\n",
        "for i, res in enumerate(optimizer.res):\n",
        "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   |    bs     |  epochs   |   loss    |    lr     | optimi... |\n",
            "-------------------------------------------------------------------------------------\n",
            "Adam Optimizer\n",
            "Cross Entropy Loss\n",
            "Loss: 0.17216931283473969\t \t Accuracy: 95.59999704360962\n",
            "Loss: 0.20090298354625702\t \t Accuracy: 95.20000219345093\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9819  \u001b[0m | \u001b[0m 250.8   \u001b[0m | \u001b[0m 2.441   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.003093\u001b[0m | \u001b[0m 1.294   \u001b[0m |\n",
            "SGD Optimizer\n",
            "Cross Entropy Loss\n",
            "Loss: 0.1523716151714325\t \t Accuracy: 95.55555582046509\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.9841  \u001b[0m | \u001b[95m 105.4   \u001b[0m | \u001b[95m 1.373   \u001b[0m | \u001b[95m 1.346   \u001b[0m | \u001b[95m 0.004028\u001b[0m | \u001b[95m 2.078   \u001b[0m |\n",
            "RMSProp Optimizer\n",
            "Cross Entropy Loss\n",
            "Loss: 0.2841380834579468\t \t Accuracy: 93.75\n",
            "Loss: 2.270498752593994\t \t Accuracy: 21.875\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.101   \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 2.069   \u001b[0m | \u001b[0m 1.44    \u001b[0m | \u001b[0m 0.007613\u001b[0m | \u001b[0m 2.775   \u001b[0m |\n",
            "RMSProp Optimizer\n",
            "Cross Entropy Loss\n",
            "Loss: 2.3266098499298096\t \t Accuracy: 11.11111119389534\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.1135  \u001b[0m | \u001b[0m 103.5   \u001b[0m | \u001b[0m 1.738   \u001b[0m | \u001b[0m 1.219   \u001b[0m | \u001b[0m 0.006548\u001b[0m | \u001b[0m 2.966   \u001b[0m |\n",
            "SGD Optimizer\n",
            "NLL Loss\n",
            "Loss: -4.290323257446289\t \t Accuracy: 12.820513546466827\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.1028  \u001b[0m | \u001b[0m 79.62   \u001b[0m | \u001b[0m 1.65    \u001b[0m | \u001b[0m 1.601   \u001b[0m | \u001b[0m 0.002448\u001b[0m | \u001b[0m 1.958   \u001b[0m |\n",
            "=====================================================================================\n",
            "{'target': 0.9841000437736511, 'params': {'bs': 105.36769045642141, 'epochs': 1.3725204227553418, 'loss': 1.3455607270430479, 'lr': 0.004027997994883633, 'optimizervalue': 2.077633468006714}}\n",
            "Iteration 0: \n",
            "\t{'target': 0.9819000363349915, 'params': {'bs': 250.82585810675315, 'epochs': 2.440648986884316, 'loss': 1.0001143748173449, 'lr': 0.003093092469055214, 'optimizervalue': 1.293511781634226}}\n",
            "Iteration 1: \n",
            "\t{'target': 0.9841000437736511, 'params': {'bs': 105.36769045642141, 'epochs': 1.3725204227553418, 'loss': 1.3455607270430479, 'lr': 0.004027997994883633, 'optimizervalue': 2.077633468006714}}\n",
            "Iteration 2: \n",
            "\t{'target': 0.10099999606609344, 'params': {'bs': 64.00350393226498, 'epochs': 2.068643182825142, 'loss': 1.440367099345832, 'lr': 0.007613231642352045, 'optimizervalue': 2.774674372156841}}\n",
            "Iteration 3: \n",
            "\t{'target': 0.11349999904632568, 'params': {'bs': 103.47072047820957, 'epochs': 1.7380537953537174, 'loss': 1.218983770581836, 'lr': 0.006547912960693306, 'optimizervalue': 2.9656970199759525}}\n",
            "Iteration 4: \n",
            "\t{'target': 0.10279999673366547, 'params': {'bs': 79.6195219532568, 'epochs': 1.6504271656649623, 'loss': 1.6007314464554383, 'lr': 0.002447615217046136, 'optimizervalue': 1.958444744573148}}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}